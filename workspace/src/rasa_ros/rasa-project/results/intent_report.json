{
  "deny": {
    "precision": 0.78,
    "recall": 0.7722772277227723,
    "f1-score": 0.7761194029850748,
    "support": 101,
    "confused_with": {
      "affirm": 12,
      "unknown": 6
    }
  },
  "ask_location": {
    "precision": 0.8590604026845637,
    "recall": 0.9481481481481482,
    "f1-score": 0.9014084507042253,
    "support": 135,
    "confused_with": {
      "ask_count": 6,
      "affirm": 1
    }
  },
  "inform": {
    "precision": 0.9342857142857143,
    "recall": 0.9058171745152355,
    "f1-score": 0.919831223628692,
    "support": 361,
    "confused_with": {
      "ask_location": 13,
      "affirm": 8
    }
  },
  "ask_count": {
    "precision": 0.9245283018867925,
    "recall": 0.9483870967741935,
    "f1-score": 0.9363057324840764,
    "support": 155,
    "confused_with": {
      "ask_location": 6,
      "inform": 2
    }
  },
  "ask_functions": {
    "precision": 0.9326923076923077,
    "recall": 0.9238095238095239,
    "f1-score": 0.9282296650717704,
    "support": 105,
    "confused_with": {
      "unknown": 3,
      "inform": 2
    }
  },
  "unknown": {
    "precision": 0.8521739130434782,
    "recall": 0.7777777777777778,
    "f1-score": 0.8132780082987553,
    "support": 126,
    "confused_with": {
      "deny": 11,
      "inform": 9
    }
  },
  "affirm": {
    "precision": 0.8494208494208494,
    "recall": 0.8979591836734694,
    "f1-score": 0.873015873015873,
    "support": 245,
    "confused_with": {
      "goodbye": 7,
      "inform": 5
    }
  },
  "goodbye": {
    "precision": 0.8260869565217391,
    "recall": 0.7524752475247525,
    "f1-score": 0.7875647668393783,
    "support": 101,
    "confused_with": {
      "greet": 11,
      "affirm": 7
    }
  },
  "thanks": {
    "precision": 0.9615384615384616,
    "recall": 0.8928571428571429,
    "f1-score": 0.9259259259259259,
    "support": 28,
    "confused_with": {
      "goodbye": 1,
      "ask_count": 1
    }
  },
  "greet": {
    "precision": 0.8695652173913043,
    "recall": 0.8860759493670886,
    "f1-score": 0.8777429467084639,
    "support": 158,
    "confused_with": {
      "affirm": 7,
      "goodbye": 4
    }
  },
  "accuracy": 0.8818481848184818,
  "macro avg": {
    "precision": 0.8789352124465211,
    "recall": 0.8705584472170104,
    "f1-score": 0.8739421995662235,
    "support": 1515
  },
  "weighted avg": {
    "precision": 0.8821756180533921,
    "recall": 0.8818481848184818,
    "f1-score": 0.8813492056338356,
    "support": 1515
  },
  "micro avg": {
    "precision": 0.8818481848184818,
    "recall": 0.8818481848184818,
    "f1-score": 0.8818481848184818,
    "support": 1515
  }
}